[
["index.html", "Generalising spatial data to 3D and VR 1 About this", " Generalising spatial data to 3D and VR Miles McBain, Michael Sumner 1 About this This workshop is for R users familiar with spatial workflows looking to explore and share their data in new mediums like VR. In introduces two new packages quadmesh and r2vr. "],
["getting-set-up.html", "2 Getting Set Up", " 2 Getting Set Up You need the latest versions of these R packages installed using devtools. devtools::install_github(‘hypertidy/quadmesh’) devtools::install_github(‘hypertidy/anglr’) devtools::install_github(‘milesmcbain/r2vr’) devtools::install_github(‘milesmcbain/r2vr.gis’) You also need an account on https://glitch.com if you have not registered one already. "],
["why-vr-r.html", "3 Why VR + R 3.1 Questions 3.2 Overview 3.3 Why are we here? 3.4 R + VR for Science 3.5 This Workshop:", " 3 Why VR + R 3.1 Questions What are our motivations for learning about how to use Virtual Reality with R? What are the use cases for VR + R? 3.2 Overview Teaching: 2 min Exercises: 5 min 3.3 Why are we here? Form groups of up to four people with your neighbours and discuss your interest in VR and R. Consider: What kind of data do you have that you would like to explore in VR? What are the best VR experiences you have had? Who would you be creating VR experiences for? 3.4 R + VR for Science The motivation for this work with R and VR is to give people who analyse data in R, like scientists, the opportunity to use that data in a virtual reality experience without having to engage with specialist 3D software developers. At ACEMS we have been investigating VR as tool for communicating science relating to the Great Barrier Reef and also eliciting expert information for spatial models. Within the context of the spatial work we found that WebVR was particularly nice as a platform for visualising spatial data, since WebVR scenes: are easy to distribute online and require no special software or hardware to view, although the viewing is enhanced by VR hardware. give the user complete control of the camera have convenient API for scripting user interactions. 3.5 This Workshop: The workshop is split into three rough acts: Intro to WebVR and A-Frame in HTML, on glitch.com General 3D representations of spatial data in R Creating WebVR scenes in R using spatial data. "],
["intro-to-webvr.html", "4 Intro to WebVR 4.1 Questions 4.2 Overview 4.3 What is WebVR? 4.4 Creating WebVR Applications 4.5 Check out A-Frame 4.6 Why A-Frame is Awesome 4.7 The A-Frame Stack", " 4 Intro to WebVR 4.1 Questions What is WebVR? What are people using WebVR for? How can I access WebVR content? How do I create WebVR content? 4.2 Overview Teaching: 5 min Exercises: 5 min 4.3 What is WebVR? The home of WebVR is: https://www.webvr.rocks where it says WebVR is a JavaScript API for creating immersive 3D, Virtual Reality experiences in your browser. WebVR is a standard. If you make an experience that complies with this standard, it is guaranteed to work across a wide variety of devices including phones, VR headsets, and personal computer browsers. This is an example of a WebVR experience: http://graphics.latimes.com/mars-gale-crater-vr/ It is a guided tour of the area of Mars where the Curiosity rover landed. A fact relevant to this workshop is that it is a piece of scientific storytelling involving spatial data. If you access it from your browser you will be guided to control the app with mouse and keyboard, if you access it from a phone, the gaze will controlled by the phone’s accelerometer. With a full VR rig you can use the hand controllers to pilot the ‘ship’. 4.4 Creating WebVR Applications There are a number of ways to build WebVR applications. As you might guess to do it from scratch involves writing lots of JavaScript. To avoid writing too much JS, tools and frameworks have emerged to make WebVR easier to program. The main contenders in this space are: WebVR export functionality built into Unity You’re a games developer who wants to make WebVR Amazon Sumerian You’re an eLearning developer who wants to make WebVR Facebook React 360 You’re a JavaScript developer who wants to make WebVR Mozilla A-Frame You’re anyone who wants to make WebVR Mozilla’s A-Frame has the largest and most active group of users. A-Frame shares features in common with an older framework called X3dom which is successor to VRML, the original declarative framework for VR on the web from the 90’s. However, an important architectural difference makes A-Frame more flexible. 4.5 Check out A-Frame On your laptop, Navigate your browser to https://aframe.io Try some of the ‘Examples’ from the left had list Try navigating to an example with your phone browser. 4.6 Why A-Frame is Awesome Let us consider the A-Frame ‘Hello WebVR’ Example: https://aframe.io/examples/showcase/helloworld/ A-Frame generalises the idea of a Web page to VR. Instead of a page, you construct a scene. The premise of the framework is that you define the static scaffolding of your scene using special HTML syntax. By static scaffolding, we mean all the geometric objects, 3D models, their positions, rotations, and colours etc. Dynamism is handled with JavaScript. The twist is that rather than writing JS yourself, you can attach behaviours or properties to items in your scene implemented in JS by other people. Much like R’s library of packages there is a growing library of A-Frame components to add all kinds of behaviour into your scenes. Behaviours can be simple, like making things move on a pre-programmed path, and also complex, like making an object behave as if acted on by gravity. This flexible approach is called ‘Entity-Component Architecture’. We’ll be discussing it further as we dive into A-Frame. 4.7 The A-Frame Stack The A-Framework is built on top of a JS 3D visualisation framework called three.js. three.js itself builds on top of the WebGL API meaning the stack looks like this: ---------- | A-Frame | | | ----------- | three.js | | | ---------- | WebGL | | | ---------- The abstraction is leaky in the sense that there are some things that can only be achieved with knowledge of the lower level frameworks. When we place R on top of that stack the same thing remains true, that is why we are spending time building knowledge of A-Frame. "],
["a-frame-concepts.html", "5 A-Frame Concepts 5.1 Questions 5.2 Overview 5.3 Anatomy of an A-Frame Scene 5.4 Config Conventions 5.5 Entity component configuration 5.6 A-Frame Documentation 5.7 Lights, Camera, Actions (controls) 5.8 Summary", " 5 A-Frame Concepts 5.1 Questions How are VR scenes described using A-Frame? How do I navigate the A-Frame Documentation? 5.2 Overview Teachin: X Min Exercises: X Min 5.3 Anatomy of an A-Frame Scene Let’s have a look at an example to get a feel for how an A-Frame scene hangs together. 5.3.1 Open an A-Frame scene on glitch.com Open this link in your browser: https://glitch.com/edit/#!/pricey-kitten Click “Remix to Edit” to create a copy of the project in your Glitch account Click “Show Live” to see what the scene looks like. Explore the scene briefly. You can use the WASD or arrow keys to move and mouse to change the camera angle. In the left hand file pane click “index.html” to open the app source. What is glitch.com? Glitch is a social coding site that makes is easy to write and host JS web apps from within your browser. A lot of A-Frame introductory examples are hosted there. Ignoring the solutions directory, we see that the scene is primary composed of a single index.html file. There is one associated JavaScript file spin.js, the purpose of which will become clear soon. 5.3.2 The header &lt;head&gt; &lt;title&gt;Hello, WebVR! - A-Frame&lt;/title&gt; &lt;meta name=&quot;description&quot; content=&quot;Hello, WebVR! - A-Frame&quot;&gt; &lt;script src=&quot;https://aframe.io/releases/0.8.0/aframe.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;spin.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/aframe-mirror-component/dist/aframe-mirror-component.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; The header of an A-Frame scene is like a normal html web page. Within it we can define the scene title and metadata. Typically an A-Frame scene will have many script calls that load JS files. In this case: &quot;https://aframe.io/releases/0.8.0/aframe.min.js&quot; is the A-Frame framework. Without it there is no VR &quot;spin.js&quot; is a local JS file that contains a component called spin &quot;https://unpkg.com/aframe-mirror-component/dist/aframe-mirror-component.min.js&quot; is a 3rd party component included from a repository. So this is roughly analogous to declaring all your R packages with library() calls at the beginning of your script. 5.3.3 The scene Everything within the a-scene tag defines the items in the scene. There are two zones: 5.3.3.1 The assets block &lt;a-assets&gt; &lt;img crossorigin=&quot;anonymous&quot; id=&quot;hadz&quot; src=&quot;https://cdn.glitch.com/aaf7e3b8-f72b-405d-a6dd-b5ba91c32622%2FJT_R_code.gif?1542275718299&quot;&gt;&lt;/img&gt; &lt;/a-assets&gt; The assets block is where media files are defined that need to be pre-loaded before the scene will be rendered. Each media file in the assets block is assigned an id that can be used to reference the asset as many times as required throughout the scene. 5.3.3.2 The entities &lt;!-- hadley box --&gt; &lt;a-box id = &quot;important&quot; position=&quot;-1 0.5 -3&quot; rotation=&quot;0 45 0&quot; shadow spin = &quot;axis: y; speed: 1.6&quot; scale=&quot;1 1 1&quot; src=&quot;#hadz&quot;&gt; &lt;/a-box&gt; &lt;!-- mirrored sphere --&gt; &lt;a-sphere position=&quot;0 1.0 -4.75&quot; radius=&quot;1.25&quot; color=&quot;#EF2D5E&quot; shadow mirror=&quot;resolution: 64; interval: 150; distance: 5000; repeat: true&quot;&gt;&lt;/a-sphere&gt; &lt;!-- plain ol&#39; yellow cylinder --&gt; &lt;a-cylinder position=&quot;1 0.75 -3&quot; radius=&quot;0.5&quot; height=&quot;1.5&quot; color=&quot;#FFC65D&quot; shadow&gt;&lt;/a-cylinder&gt; &lt;!-- floor --&gt; &lt;a-plane position=&quot;0 0 -4&quot; rotation=&quot;-90 0 0&quot; width=&quot;4&quot; height=&quot;4&quot; color=&quot;#7BC8A4&quot; shadow&gt;&lt;/a-plane&gt; &lt;!-- backboard --&gt; &lt;a-plane position=&quot;0 2 -6&quot; rotation=&quot;0 0 0&quot; width=&quot;4&quot; height=&quot;4&quot; color=&quot;#7BC8A4&quot; shadow&gt;&lt;/a-plane&gt; &lt;!-- the background colour --&gt; &lt;a-sky color=&quot;#ECECEC&quot;&gt;&lt;/a-sky&gt; The objects that comprise the scene are called entities. Here they are defined by the tags a-box, a-sphere, a-plane etc. Within the entity tags, the attributes define configuration for components. This configuration is enclosed in quotes and can contain multiple properties e.g.: spin = &quot;axis: y; speed: 1.6. 5.4 Config Conventions 5.4.1 Coordinate system To fully comprehend the scene configuration you need to know the coordinate system. In VR, The standard unit is meters, which are scaled in VR space to look like meters in real space. In the coordinate system X points “right”, Y points “up”, and Z points toward the camera. Like so: | Y + | |_ _ X + / / Z + This can be confusing when using spatial data since the spatial convention is typically to have Z pointing “up from the table” and Y pointing “up the page” 5.4.2 Vectors Vectors of X,Y,Z are written as space delimited triples like so: * position=&quot;-1 0.5 -4&quot; * rotation=&quot;-90 0 0&quot; In the case of rotation the each vector component specifies degrees rotation clockwise about an entity’s axis. 5.4.3 Applying Rotation The green floor has a rotation of -90 0 0. Locate it in the entity configuration. Try subbing in large and small, positive and negative values for -90 and viewing the results. Which axis is the rotation around? Where is this axis of rotation on the floor entity? 5.4.4 Camera Position Unless otherwise specified the camera as placed at the origin, at a height of 1.6m, looking in the negative direction on the Z axis. Camera position and angle can be configured by adding an a-camera entity to the scene. 5.4.5 Tweaking a scene We’ll dive deeper into entity configuration soon, but for now let’s see what we can do intuitively based on what’s in front of us in the scene HTML. Make the following tweaks: Insert a new a-camera entity. Place it at the origin, at roughly your eye height by setting its position. Alter the yellow cylinder so that it is lying down on it’s side on the floor. Remove the mirror effect from the sphere in the back. Reverse the spin direction of the cube. 5.5 Entity component configuration An A-Frame scene is made up completely of entities that may reference assets. It the configuration we have seen so far it looks like entities have many types: ‘box’, ‘sphere’, ‘cylinder’, ‘plane’ etc - But this is actually an illusion. The ‘type’ or nature of all entities is completely determined by their components. But what are their components?! There a couple of ways to write entities and what we have seen so far was short hand to save typing. The shorthand is convenient but it can make seeing how the entity is composed from components less clear. Consider the yellow cylinder in it’s initial position and orientation: &lt;a-cylinder position=&quot;1 0.75 -3&quot; radius=&quot;0.5&quot; height=&quot;1.5&quot; color=&quot;#FFC65D&quot; shadow&gt;&lt;/a-cylinder&gt; That definition is a shorthand for this: &lt;a-entity position=&quot;1 0.75 -3&quot; geometry=&quot;primitive: cylinder; height: 1.5; radius: 0.5&quot; material=&quot;color: #FFC65D&quot; shadow&gt;&lt;/a-entity&gt; Where each component has been placed on a new line for emphasis. Now it is clear that the cylinder we see is an entity with four components: a position component that places it in spaces a geometry component that determines its shape a material component that sets the colour and reflective properties of its surfaces a shadow component that lets the entity cast shadows and have shadows casted on it. Notice how component properties are configured as string of key value pairs, delimited by : and separated by ;. We can consult the A-Frame documentation entry for a-cylinder to see how its attributes map to properties of the underlying components: https://aframe.io/docs/0.8.0/primitives/a-cylinder.html 5.5.1 The id One useful attribute that is not a component is id. It can be applied to any entity or asset as a means to refer to them in the configuration of other entities. In our example we referred to an asset with id ‘hadz’ in the src attribute of the box, to use the image as a texture. When referring to ids they are prefixed with a #, eg #hadz. 5.5.2 Switching to long form Still in the same demo scene, replace a-box with an a-entity that defines all components explicitly. To this you will need to know that: position is a component rotation is a component shadow is a component spin is a component scale is a component src is NOT a component, it is an attribute. The A-Frame documentation entry for a-box may help: https://aframe.io/docs/0.8.0/primitives/a-box.html 5.6 A-Frame Documentation We’ve mentioned the A-Frame documentation now a couple of times and it is worth quickly becoming familiar with how to navigate it, because it is an extensive and well put together resource. The documentation can be found at: https://aframe.io/docs Now let’s use the documentation to fix a problem with our scene: If you walk around the back of the objects you will notice that the ‘backboard’ turns invisible. This is because by default A-Frame only renders the ‘front’ side of a 3D model or geometry. For closed shapes this is fine, but for open shapes this disconcerting disappearing effect will occur. If we type ‘material’ into the search box in the top left of the docs website we will get a list of suggestions, choose the first one. If we look though the properties of the material component, we will find one that can address this issue: side: double. The documentation also has some vignette style content through the ‘Guides’ link. 5.7 Lights, Camera, Actions (controls) We’ve covered entities and components, but there are a couple special components that really make things ‘work’: The camera: The user’s viewport Lights: illumination for the scene. Lighting can have a huge effect on scene mood and perceived realism. Controls: The control scheme affects how the user can interact with the your VR creation. 5.7.1 Lighting If no entities with light components are configured, the framework will automatically inject two lights: an ambient light source and a point light source. A point light source emits light outward in all directions. It can cast shadows. An ambient light source increases the brightness of all surfaces in the scene. It has no origin or direction and so cannot cast shadows. These two together make a reasonable approximation of an outdoor lighting model. The point source models the sun (or moon), and the ambient source models indirect light reflected light from various sources. 5.7.2 Lighting it up Here’s a little example to showcase how light sources combined create mood and a realistic look. We have a drab night-time scene with only ambient lighting at: https://glitch.com/edit/#!/giddy-taxi Open the link and click the ‘Remix to Edit’ button. Show the live scene. The scene contains only a ambient light. Try various intensity settings and observe the effect on the scene. Add a new light source to represent the Moon. Position it 40 meters high, 30 meters in front the viewer. Make the type ‘point’ Set castShadow ‘true’ set the intensity to 0.5 (50%) Review the scene. Tweak the intensity settings of the two lights until it feels right. Add a geometry component and a material component to the ‘Moon’ so it becomes visible. use a sphere with radius ‘2’. set the color to ‘white’. Add this config to the material component: shader: flat - This will stop it dimming with distance. Review your light intensity settings now you have a bright full Moon in the sky and set appropriately. The documentation entries for ‘light’ and ‘geometry &gt; sphere’ may assist you. Bonus! For fun try animating the moon using a third party library. Here’s a template of the component config: animation=&quot;property: position; to: &lt;END_POSITION&gt;; dir: alternate; loop: true; dur: &lt;ANIMATION_DURATION_ms&gt;&quot; 5.7.3 Cameras and Controls Camera and control components tend to be used together. As mentioned the scene gets a default camera if none is configured. The config for that camera looks roughly like this: &lt;a-entity camera wasd-controls look-controls&gt;&lt;/a-entity&gt; It has two control components that allow inputs to rotate and move the camera: wasd-controls listens for keyboard input and moves the camera accordingly look-controls listens for movement from a phone or headset accelerometer, or a mouse. It can also respond to swipes on a touch screen as per a mouse. Components for hand controllers are also attached to the camera entity for interactivity. The most general way to do interactivity is to have entities things that react to being looked at, this way even mobile users can interact with the scene. This can control is added with the cursor component. We’ll consider an example that does this later in the session. Like other components, controls have properties that alter how they work. 5.7.4 Learn to fly Still in the context of our moonlit night scene: Examine the documentation for wasd-controls Add a camera entity to the scene with look-controls, and wasd-controls configured to: move the camera twice as fast as the default allow the camera to ‘fly’, instead of being locked into a single plane. Notice how when moving fast clicking and dragging to turn becomes infeasible. It is possible to configure the mouse to control the camera ‘First Person Shooter’ style with community components. 5.8 Summary VR conventions Composing entities from components Configuring components Navigating the A-Frame documentation Lights Camera Controls "],
["developing-with-a-frame.html", "6 Developing with A-Frame 6.1 Questions 6.2 Overview 6.3 A-Frame Dev Tools 6.4 Common Patterns 6.5 Finally 6.6 Summary", " 6 Developing with A-Frame 6.1 Questions How do I make scenes interactive? How do I find and use third party components and assets? How do I debug A-Frame Scenes? How can I move scenes off glitch? 6.2 Overview Teaching: X minutes Exercises: X minutes 6.3 A-Frame Dev Tools Now that we have the most of the basic concepts down it’s worth discussing practical aspects of developing with A-Frame. This will assist with tackling more ambitious examples. This applies to scenes built in R as well as natively. We’ll use this glitch project to demonstrate: https://glitch.com/edit/#!/meteor-cappelletti 6.3.1 Browser Dev Tools The code looks similar to the first example we saw, with the notable addition of a duck ‘gltf-model’. But where is that duck in the scene? Something has gone wrong. The first place to check when something unexpected like this happens is the console in the browser. It can be opened with: Ctrl + Shift + j in Chrome Ctrl + Shift + k in Firefox You’re looking in the right place if you can spot messages like these: A-Frame Version: 0.8.0 (Date 2018-03-11, Commit #82934b02) index.js:86 three Version: github:dmarcos/three.js#r90fixPose index.js:87 WebVR Polyfill Version: ^0.9.40 These are normal messages that confirm WebVR components completed loading. Warning messages may be present but these are usually safe to ignore. Errors are usually illuminating. In Chrome, this scene throws this error: /duck:1 Failed to load resource: the server responded with a status of 404 () ‘duck’ could not be found. Which is strange because the server should be looking for a 3D model file e.g. ‘duck.gltf’. 6.3.1.1 Duck Tales Pt. 1 Can you spot the mistake that is generating the error? Compare the way the duck model is loaded to the Hadley gif. 6.3.2 A-Frame Inspector We’ve fixed the error but still no duck. When trying to understand a scene, the built-in ‘A-Frame Inspector’ is useful for browsing entities in the scene and viewing their properties. While viewing any A-Frame VR scene, Open the A-Frame inspector with Ctrl + Alt + i. It should look similar to this: The list down the left hand side shows the entities in the scene. Clicking on them will open an area on the right where their properties can be viewed and also manipulated. The effect of updating properties will be shown in real time. This is very useful to discover the perfect settings fiddly things such as positions, colours, and lighting. 6.3.2.1 Duck Tales Pt. 2 Using the A-Frame inspector, discover where the Duck is in this scene. 1.Use the information about the duck in the a-glf-model to change the Duck’s position, scale, and rotation such that it is perched on top of the yellow cylinder. One new thing you need to know to do this is that the scale component is configured with a vector of scale factors to apply along each axis of the entity independently. 6.3.3 Aside: Models and the scaling issue The duck example was contrived but it reflects a common occurrence when working with spatial data. As we will see later, the method of importing spatial data into VR is to generate a 3D mesh from the data, which is loaded as if it were a regular 3D model (like our duck). Since spatial data usually have to be projected to a flat surface with units in meters, they become very large in VR, where meters are roughly equivalent to the real world. Using the scale component is essential to making the data viewable. 6.4 Common Patterns This section describes common patterns that are useful to know when constructing scenes. 6.4.1 Nesting entities Managing all the properties of all the components of all the entities in a dynamic scene could easily become laborious if each had to be treated individually. A-Frame’s answer to this is to allow nested entities to inherit configuration from their parents. To use the duck and yellow cylinder as an example, a nested configuration looks like this: &lt;a-cylinder position=&quot;1 0.75 -3&quot; radius=&quot;0.5&quot; height=&quot;1.5&quot; color=&quot;#FFC65D&quot; shadow&gt; &lt;a-gltf-model src=&quot;#duck&quot; position=&quot;0 0.5 0&quot; rotation=&quot;0 -100 0&quot; scale=&quot;1 1 1&quot;&gt;&lt;/a-gltf-modell&gt; &lt;/a-cylinder&gt; The duck model entity is nested within the closing &lt;/a-cylinder&gt; tag of the cylinder entity. Notice the duck’s position is now relative to that of the cylinder. If you preview this configuration you will also notice the duck has inherited the shadow attribute from the cylinder and now casts a shadow onto it. Changing the position, rotating, or scaling the cylinder will also affect the duck. the duck’s own properties are applied relative to that of the cylinder 6.4.1.1 Camera Rig A common application of the nesting pattern is the creation of a camera rig with a custom controls and a cursor. This depends on a controls component from a popular library of components called A-Frame Extras This library has already been loaded in the header. Your task is to: Take the ‘Basic movement example’ from A-Frame Extras Movement Controls and embed it in your scene. Within the entity that define the camera, nest an a-cursor entity. If you’re successful you should see a small black circle in the centre of your view in the scene. More than just aesthetics, the cursor is useful for driving interactivity. 6.4.2 Interactivity Interactivity in A-Frame borrows again from the web page model. A-Frame has an event system, which allows components to register handlers for certain types of events. Components may also emit events ‘on’ other entities. An example might make this clear: The cursor component can emit events ‘on’ entities that: It has just started pointing at (mouseover) It has just stopped pointing at (mouseleave) It was pointing at while a button was pressed (click) It has been pointing at for a set amount of time (click) This is called ‘fusing’ This list is not exhaustive but it gives you an idea. An entity that receives one of these events can execute code in response to it if it has a defined handler for an event of that name. Handlers can be attached with components. There is one useful 3rd party component who’s sole purpose is to make it easy to register handlers for events: The event-set component. We’ll use this in an example of the interactivity system: To the Hadley-box, we add the component: event-set__mouseenter=&quot;material.shader: gif&quot; To the duck, we add the components: event-set__mouseenter=&quot;spin.speed: 3.14&quot; event-set__mouseleave=&quot;spin.speed: 0&quot; So now when we look at Hadley, he will start typing, and we look at the duck, it will rotate. What has happened is that the events mouse enter and leave events emitted by cursor have had handlers added for them by event-set that change the value of component properties, changing the entity behaviour. There is a bit a of a blip when the Hadley animation starts and this is due to us starting the animation in a rough way by changing the entity’s shader, requiring it to be redrawn. A small amount of JS could have made this transition smooth, but it is outside the scope of this introduction. 6.5 Finally 6.5.1 Working with A-Frame outside of Glitch If you try to copy a glitch project down to your local machine and open the index.html file in a browser, it will not work properly. This is because the JS files and assets need to be served using the appropriate protocol or your browser will deem them a security risk and block them. That is to say the scene needs to be served by a web server. There are numerous tools to do this available. But if you create A-Frame via R, this is handled for you. 6.5.2 Finding Community Components In the examples we have used a number of community components, imported via script tags in the header. There are two good places to find community components: The A-Frame Registry Searching for A-Frame in the Node Package Manager (npm) Components will usually provide an example that shows url to use to import them. 6.6 Summary A-Frame inspector Chrome devtools nested entities interactivity serving your scene Community components "],
["d-and-mesh-forms-of-spatial-data.html", "7 3D and mesh forms of spatial data 7.1 Questions 7.2 Overview 7.3 What is a mesh? 7.4 Geospatial data 7.5 Primitives 7.6 A raster is a mesh (implicitly) 7.7 the rgl mesh3d format 7.8 Quadmesh and coordinate systems 7.9 Summary 7.10 rgl miscellanea", " 7 3D and mesh forms of spatial data 7.1 Questions What are meshes and topology? what is the relationship of meshes to geospatial raster/vector data? 7.2 Overview Teaching: 20 min Exercises: 5 min 7.3 What is a mesh? Key ideas! Topology: the shape of things and their relationships. Geometry: the where of things. Indexing: the link between topology and geometry. Traditional spatial data tends to confuse topology and geometry. These are motiving problems behind my interest in these ideas. Lossless reprojection Topology fixes Tracks and point clouds Visualization Topology vs. geometry This line has 1-dimensional topology depicted in 3-dimensional geometry. rgl.clear() library(rgl) (geometry &lt;- cbind(x = c(0, 0.5, 1), y = c(0, 0.5, 1), z = c(0, 0, 0.8))) ## x y z ## [1,] 0.0 0.0 0.0 ## [2,] 0.5 0.5 0.0 ## [3,] 1.0 1.0 0.8 (topology1 &lt;- rbind(.v0 = c(1, 2), .v1 = c(2, 3))) ## [,1] [,2] ## .v0 1 2 ## .v1 2 3 lines3d(geometry[t(topology1), ], lwd = 3, col = &quot;firebrick&quot;) material3d(col = &quot;black&quot;) axis3d(&quot;x&quot;) axis3d(&quot;y&quot;) axis3d(&quot;z&quot;) title3d(xlab = &quot;x&quot;, ylab = &quot;y&quot;, zlab = &quot;z&quot;) quads3d(cbind(c(0, 1, 1, 0), c(0, 0, 1, 1), c(0, 0, 0, 0) - 0.01), col=&quot;gray&quot;) rglwidget() This triangle has 2-dimensional topology depicted in 3-dimensional geometry. (topology2 &lt;- rbind(.v0 = c(1, 2, 1), .v1 = c(2, 3, 1))) ## [,1] [,2] [,3] ## .v0 1 2 1 ## .v1 2 3 1 triangles3d(geometry[t(topology2), ], col = &quot;firebrick&quot;) material3d(col = &quot;black&quot;) axis3d(&quot;x&quot;) axis3d(&quot;y&quot;) axis3d(&quot;z&quot;) title3d(xlab = &quot;x&quot;, ylab = &quot;y&quot;, zlab = &quot;z&quot;) quads3d(cbind(c(0, 1, 1, 0), c(0, 0, 1, 1), c(0, 0, 0, 0)), col=&quot;gray&quot;) rglwidget() 7.4 Geospatial data raster vector These are traditionally kept separate, but in computer graphics the distinction starts to disappear. What is a raster? A layer of neighbouring rectangles? Or a continuous fields between points? (Lots of ways to infer the field, including this very poor one). What is a polygon? A series of grouped paths? What’s in the middle? ## Warning in st_centroid.sf(tri): st_centroid assumes attributes are constant ## over geometries of x The fill we see in traditional 2D graphics is a trick!!. Search: it’s not what you draw it’s what you not draw ~Paul Murrell Technically the trick comes in two types, either the even-odd or winding rule, and this trick is not part of this workshop. The graphics engine uses this rule to draw a pixel if it has been encircled an even or odd number of times, or using a rule about in which direction it was encircled. It happens deep in the graphics. Where it does matter is for the concept of orientation, and 3D graphics do care about the direction that triangles are wound (consider that reversing the direction is like flipping the triangle in place in terms of how some algorithms behave …). What’s the fill? In 3D, and to fill our polygons properly as data - we need primitives. 7.5 Primitives Terminology alert! (This is a working definition of primitive, not everyone agrees.) Point - a single coordinate is a 0-dimensional primitive (vertex, coordinate) Line - a line segment between two coordinates is a 1-dimensional primitive (edge, segment) Triangle - a triangle joining three coordinates is 2-dimensional primitive Topology ain’t geometry (This is topological dimension. Every one of these types of shape can be depicted within a geometric space that is equal to or higher than the topological dimension.) We will have a matrix of vertices and a matrix of primitive indices. Quads and triangles are generally called faces, line segments are alternatively called edges. All are primitives in computer graphics, but we’ll also see the term finite element used. Topology can be 3D (tetrahedron) - imagine volumetric fill versus planar faces bounding a volume. Geometry can be 4D, X, Y, Z, T - or any dimensionality we need. To fill our polygon we need triangles. data(&quot;minimal_mesh&quot;, package = &quot;silicate&quot;) tri &lt;- sf::st_cast(sfdct::ct_triangulate(minimal_mesh, a = 0.01, D = TRUE)) ## Warning in st_cast.sf(sfdct::ct_triangulate(minimal_mesh, a = 0.01, D = ## TRUE)): repeating attributes for all sub-geometries for which they may not ## be constant plot(tri) Note that if we turn off the border, we don’t notice the difference. plot(tri, border = NA) No tricky winding or even-odd rule to worry about, but we have lost our boundary around each distinct shape - we could find them by finding edges within a shape that are not shared by two triangles … plot(tri, border = NA, col = rainbow(10)) ## Warning in plot.sf(tri, border = NA, col = rainbow(10)): col is not of ## length 1 or nrow(x): colors will be recycled; use pal to specify a color ## palette Raster and vector are not a strong distinction when it comes to meshes A raster is a very simple version of a mesh. When we store a raster we need the equivalent of number of columns and rows the extent in the coordinate system used (xmin, xmax, ymin, ymax) the coordinate system the cell values! (ncols * nrows of them) In in computer graphics we store the corner coordinates ((ncols + 1) * (nrows + 1) of them) an index, 4 indices for every quad specify the coordinates groupings, what quads belong to which objects the coordinate system (hopefully) It’s the same for a triangular mesh. the corner coordinates an index, 3 indices for every triangle groupings, what triangles belong to which objects (features) the coordinate system (hopefully) And lines are the end point coordinates of each line segment (or edge) an index, 2 indices for every segment groupings, what line segments belong to which objects (features) the coordinate system (hopefully) 7.6 A raster is a mesh (implicitly) The simplest kind of mesh is a basic raster. Consider the matrix the matrix (used above). m &lt;- matrix(c(seq(0, 0.5, length = 5), seq(0.375, 0, length = 4)), 3) On its own this matrix has absolutely nothing to do with spatial data, it is literally a collection of 9 numeric values in a given order, and by the magic of programming we’ve nominated a shape of 3x3. We can’t help but think about this shape spatially however, but there’s a problem. Does each element occupy space or should we consider them to be infinitesimal locations? R provides either interpretation (to simplify this story we nominate locations for the rows and columns explicitly). When considered as an image, each matrix element occupies a certain space in width and height, but when considered as a point set the numbers simply float at the given locations. Which is correct? (Spoiler: Both are correct, it simply depends what we are doing.) x &lt;- seq(1, nrow(m)) - 0.5 y &lt;- seq(1, ncol(m)) - 0.5 image(x, y, m, col = colpal()) text(expand.grid(x, y), lab = m[]) The raster package defaults to the image interpretation and helpfully assumes the values are nominally at the centre points as shown above. We have to nominate the extent or we end up in 0,1 range, we also have to invert the order of the values because raster counts from the top of the page and R’s matrix uses column-major order. library(raster) (r &lt;- raster(t(m[, ncol(m):1]), xmn = 0, xmx =ncol(m), ymn = 0, ymx = nrow(m))) ## class : RasterLayer ## dimensions : 3, 3, 9 (nrow, ncol, ncell) ## resolution : 1, 1 (x, y) ## extent : 0, 3, 0, 3 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## data source : in memory ## names : layer ## values : 0, 0.5 (min, max) R’s image and rasters in general are so efficient because they only store this minimal amount of information: the actual data values, and the extent and dimensions of the space they occur in. If we had to store the centre coordinate of every cell, or worse the corner coordinates then the data storage goes up dramatically. Every software that deals well with these kinds of data has to treat these coordinates as implicit. We can easily expand the centre coordinates. xyz &lt;- as.data.frame(r, xy = TRUE) head(xyz) ## x y layer ## 1 0.5 2.5 0.250 ## 2 1.5 2.5 0.125 ## 3 2.5 2.5 0.000 ## 4 0.5 1.5 0.375 ## 5 1.5 1.5 0.500 ## 6 2.5 1.5 0.375 tail(xyz) ## x y layer ## 4 0.5 1.5 0.375 ## 5 1.5 1.5 0.500 ## 6 2.5 1.5 0.375 ## 7 0.5 0.5 0.000 ## 8 1.5 0.5 0.125 ## 9 2.5 0.5 0.250 but to expand the corners we have to jump through some hoops and even then we get every instance of the corners, not only for each cell but to explicitly close the cell as a polygon. as(as(raster::rasterToPolygons(r), &quot;SpatialLinesDataFrame&quot;), &quot;SpatialPointsDataFrame&quot;) ## class : SpatialPointsDataFrame ## features : 45 ## extent : 0, 3, 0, 3 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## variables : 4 ## names : layer, Lines.NR, Lines.ID, Line.NR ## min values : 0.000, 1, 1, 1 ## max values : 0.500, 9, 9, 1 There are only 20 unique coordinates at the corners, which is where quadmesh comes in. 7.7 the rgl mesh3d format Rgl is the OpenGL package in R. A classic computer graphics data model called mesh3d, it’s not widely used but is very powerful. You can visualize a mesh3d model with shade3d(), all the aesthetics, material properties, geometry and topology can be attached to the model itself as data. It supports two kinds of primitives quads and triangles. Quads are a funny case, usually carried by two triangles (at least implicitly) but they are an important computer graphics element. library(quadmesh) qm &lt;- quadmesh(r) str(qm) ## List of 7 ## $ vb : num [1:4, 1:16] 1.49e-08 3.00 3.12e-01 1.00 1.00 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;x&quot; &quot;y&quot; &quot;z&quot; &quot;1&quot; ## .. ..$ : NULL ## $ ib : int [1:4, 1:9] 1 2 6 5 2 3 7 6 3 4 ... ## $ primitivetype : chr &quot;quad&quot; ## $ material : list() ## $ normals : NULL ## $ texcoords : NULL ## $ raster_metadata:List of 7 ## ..$ xmn : num 0 ## ..$ xmx : num 3 ## ..$ ymn : num 0 ## ..$ ymx : num 3 ## ..$ ncols: int 3 ## ..$ nrows: int 3 ## ..$ crs : chr NA ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;mesh3d&quot; &quot;shape3d&quot; This is a mysterious seeming data structure, it is the mesh3d type of the ‘rgl’ package, rarely seen in the wild. The structure is vb, the coordinates of the mesh - these are the actual corner coordinates from the input raster. image(r, col = colpal()) op &lt;- par(xpd = NA) text(t(qm$vb), lab = 1:ncol(qm$vb)) par(op) Notice how these are unique coordinates, there’s no simple relationship between the cell and its value and its four corners. This is because they are shared between neighbouring cells. The relationship is stored in the ib array, this has four rows one for each corner of each cell. There are 12 cells and each has four coordinates from the shared vertex pool. The cells are defined in the order they occur in raster. qm$ib ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 2 3 5 6 7 9 10 11 ## [2,] 2 3 4 6 7 8 10 11 12 ## [3,] 6 7 8 10 11 12 14 15 16 ## [4,] 5 6 7 9 10 11 13 14 15 It works directly with rgl function, and can be used in more raw form. rgl.clear() library(rgl) shade3d(qm, col = &quot;firebrick&quot;) rglwidget() rgl.clear() quads3d(t(qm$vb)[qm$ib,], col = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)[qm$ib %% 2 + 1]) rglwidget() rgl.clear() quads3d(t(qm$vb)[qm$ib,], col = rep(c(&quot;firebrick&quot;, &quot;dodgerblue&quot;), each = 4)) rglwidget() The situation for triangles is much the same, but we have it for the triangle index rather than ib for the quad index. In both cases the geometry is in the vb matrix. Models can have both quads and triangles, using the same set of vertices. Pop out to real example with quadmesh. 7.8 Quadmesh and coordinate systems Run this code Think about what is wrong with the scene. What can we do about the ugly plot? library(quadmesh) qm1 &lt;- quadmesh(crop(worldll, etopo)) qm1$vb[3, ] &lt;- raster::extract(etopo, t(qm1$vb[1:2, ])) library(rgl) rgl.clear() shade3d(qm1, col = &quot;white&quot;) ## run this only if you are in a web rstudio rglwidget() Run this code Can you explain why we multiply the Etopo2 terrain elevation by 20? What are alternatives we could use? qm2 &lt;- qm1 qm2$vb[3, ] &lt;- qm2$vb[3, ] * 20 qm2$vb[1:3, ] &lt;- t(llh2xyz(t(qm2$vb[1:3, ]))) rgl.clear() shade3d(qm2, col = &quot;white&quot;, specular = &quot;black&quot;) aspect3d(1, 1, 0.5) ## run this only if you are in a web rstudio rglwidget() There’s still a problem with what quadmesh can do. # two ways to think about raster data library(sf) p &lt;- spex::polygonize(r) p$color_ &lt;- colourvalues::colour_values(p$layer, palette = t(col2rgb(palr::bathyDeepPal(10)))) plot(st_geometry(p), col = p$color_) library(rgl) library(anglr) library(silicate) library(quadmesh) tri &lt;- copy_down(TRI(spex::polygonize(disaggregate(r, 4))), &quot;layer&quot;) tri$object$color_ &lt;- colourvalues::colour_values(tri$object$layer) tmp &lt;- plot3d(tri) rgl.pop() rgl.clear() shade3d(tmp, alpha = 0.5, specular = &quot;black&quot;, col = &quot;grey&quot;) #wire3d(tmp, col = &quot;black&quot;, lwd = 2) rgl::aspect3d(1, 1, .2) bg3d(&quot;lightgrey&quot;) # qm &lt;- quadmesh::quadmesh(r) ## the point of this part is to show the quads don&#39;t carry the z information well (it&#39;s ok with very fine quads) rgl::wire3d(qm, lwd = 8, col = &quot;green&quot;) ptri &lt;- geometry::delaunayn(coordinates(r)) ## ## PLEASE NOTE: As of version 0.3-5, no degenerate (zero area) ## regions are returned with the &quot;Qt&quot; option since the R ## code removes them from the triangulation. ## See help(&quot;delaunayn&quot;). t3d &lt;- structure(list(vb = t(cbind(coordinates(r), values(r), 1)), it = t(ptri), primitivetype = &quot;triangle&quot;, material = list()), normals = NULL, texcoords = NULL, class = c(&quot;mesh3d&quot;, &quot;shape3d&quot;)) rgl::wire3d(t3d, lwd = 10, col = &quot;black&quot;) rgl::rglwidget() The primary means to create this format from a raster is for 3D plotting, but because we have access to the coordinate directly it provides other uses. We can transform the coordinates (i.e. a map projection) or manipulate them and augment the Z value (for example) in flexible ways. (The usual way of driving rgl grid surfaces is rgl.surface but this is limited to the centre-point interpretation only - more than the x, y, z list interface as image() is, i.e. it can take an individual x,y pair for every cell, but it cannot properly represent the cell-as-area as image can. For this we need to use shade3d, and actual meshd3d types in rgl). 7.9 Summary Topology and geometry are independently dimensional Meshes include a matrix of vertices and a matrix of indices In mesh-terms rasters and polygons aren’t so different 7.10 rgl miscellanea Most examples around use rgl.surface, but I am less familiar with that. The thing3d are the higher-level functions in rgl, and the rgl.thing functions are lower-level (recommended not to mix them in usage). rayshader in particular, has extremely compelling outputs, but it uses the lower level rgl.surface and doesn’t maintain the geographic coordinates, so I see it mostly as a texture-generator (but watch its development!). rgl.surface can take X and Y matrices, so you can trivially reproject these data and wrap them around a sphere - I only learnt this recently. WATCH OUT The single most confusing thing I found with mesh3d was homogeneous coordinates. This is a fourth value on each vertex, making it X, Y, Z, H and because it’s stored transpose, the ib and the vb matrices have the same number of rows (4). The H loosely corresponds to “zoom”, for our purposes set H = 1. (If set to 0 no data will be visible, and a package of mine had this bug until last week.) "],
["transforming-spatial-data-to-3d-forms.html", "8 Transforming spatial data to 3D forms 8.1 Questions 8.2 Overview 8.3 Transforming Spatial Data using modular 3D tools 8.4 On CRAN 8.5 Quadmesh. 8.6 Stuff not on CRAN! 8.7 Triangles or quads in hypertidy (WIP)", " 8 Transforming spatial data to 3D forms 8.1 Questions What tools exist to convert data to meshes in R? 8.2 Overview Teaching: 10 min 8.3 Transforming Spatial Data using modular 3D tools Tell us if you use 3D visualization or meshes in analysis. This is hard! There’s a few scattered tools. 8.4 On CRAN rgl::triangulate and decido::earcut will triangulate polygons with holes but only suitable for plane-filling, because ugly triangles, no control over size and shape. RTriangle::triangulate (and sfdct::ct_triangulate) do high-quality “near-Delaunay” triangulations quadmesh::quadmesh to create rgl-ready mesh3d from a raster mapview::cubeView does very compelling interactive raster-cube visualization SymbolixAU/mapdeck tylermorganwall/rayshader Side note: there are many triangulation algorithms and many packages in R, but we need constrained triangulation to preserve all input edges - only a handful can do that, and RTriangle is the king (with a problematic license). 8.5 Quadmesh. library(quadmesh) quadmesh(anyRasterDEM, texture = anyRasterRGB) rgl::shade3d() ## play with aspect3d, light3d, ... Triangulations, sfdct is no good because it’s very inefficient. sf is simply not suitable for mesh (a.k.a. indexed) forms of data. 8.6 Stuff not on CRAN! hypertidy/silicate, hypertidy/anglr - these are evolving together this workshop! https://github.com/MilesMcBain/gis_vs_web3D coolbutuseless/threed 8.7 Triangles or quads in hypertidy (WIP) This is my work-in-progress approach to meshing any data structure. ## devtools::install_github(&quot;hypertidy/anglr&quot;) ## devtools::install_github(&quot;hypertidy/silicate&quot;) library(anglr) triangles &lt;- copy_down(TRI(anySFpolygon), anyRasterDEM) mesh &lt;- plot3d(triangles) "],
["spatial-data-to-vr-via-r.html", "9 Spatial Data to VR via R 9.1 Questions 9.2 Overview 9.3 Motivation 9.4 R to VR 9.5 Spatial data in VR", " 9 Spatial Data to VR via R 9.1 Questions How do I create an A-Frame scene with R? What kinds of data can we represent in A-Frame? How do I render spatial data in VR? 9.2 Overview Teaching: 30min Exercises: 5min 9.3 Motivation So now you’ve had a taste of the A-Frame framework and marshalling spatial data to 3D in R you’re in better position to undstand why defining VR scenes from R is a beneficial thing. Before this capability existed the workflow for VR scenes was roughly this: Repeat until ‘done’: Build 3D models and JSON data in R for export to VR Define VR in HTML/JS Serve scene Discover limitation or bug The process is naturally iterative but the speed of iteration is frustratingly slow due to context switching from the R environment to Web environment. It also leads to a nasty anti-pattern where data names and calculation results from R make their way into web land as magical constants, slowing the process even further when these need to change. 9.4 R to VR The tools that exist in R allow you to mix R code with the VR abstraction provided by A-Frame. They do not provide a higher level abstraction. To use an R analogy: grid is a low level graphics framework that gives you the power to draw anything you can imagine using 2D geometric primitives. It is up to you write functions that map your data to those primitives. ggplot2 is a popular visualisation package that does exactly this. If you’re using VR tools in R you’re going to be working with low level VR primitives. Do not expect ggplot2 level magic. A typical scene will be hundreds of lines of code, as opposed to say tens with ggplot2. The saving grace is that most of those lines of code will be about declaring simple primitive objects and interactions which are not overly complex. This is a natural situation since the domain of VR visualisations is not well understood right now. Through working with VR you will begin to see the common tasks important to your domain and if we’re lucky you might even write a package to help others do them. There are currently two packages that allow you create A-Frame scenes in R: r2vr - used in this workshop aframer They have different capabilities, APIs, and are not interoperable. 9.4.1 r2vr Hello World Here’s a familiar scene constructed with r2vr, we’ll build some simpler examples soon, this is just to compare and contrast the syntax: library(r2vr) ## Configure scene js_libs &lt;- list(&quot;https://unpkg.com/aframe-animation-component@^4.1.2/dist/aframe-animation-component.min.js&quot;, &quot;https://unpkg.com/aframe-mirror-component/dist/aframe-mirror-component.min.js&quot; ) hadley_gif &lt;- a_asset(.tag = &quot;img&quot;, src = &quot;./figs/JT_R_code.gif&quot;, id = &quot;hadz&quot;) box &lt;- a_entity(.tag = &quot;box&quot;, position = c(-1, 0.5, -3), rotaion = c(0, 45, 0), src = hadley_gif, shadow = &quot;&quot;, animation = list(property = &quot;rotation&quot;, to = c(0, 360, 0), dur = 2000, loop = TRUE, easing = &quot;linear&quot;)) sphere &lt;- a_entity(.tag = &quot;sphere&quot;, position = c(0, 1.25, -5), radius = 1.25, color = &quot;#EF2D5E&quot;, shadow = &quot;&quot;, mirror = list(resolution = 64, interval = 150, distance = 5000, `repeat` = TRUE)) cylinder &lt;- a_entity(.tag = &quot;cylinder&quot;, position = c(1, 0.75, -3), radius = 0.5, height = 1.5, color = &quot;#FFC65D&quot;, shadow = &quot;&quot;) floor &lt;- a_entity(.tag = &quot;plane&quot;, position = c(0, 0, -4), rotation = c(-90, 0, 0), width = 4, height = 4, color = &quot;#7BC8A4&quot;, shadow = &quot;&quot;) backboard &lt;- a_entity(.tag = &quot;plane&quot;, position = c(0, 2, -6), rotation = c(0, 0, 0), width = 4, height = 4, color = &quot;#7BC8A4&quot;, shadow = &quot;&quot;) sky &lt;- a_entity(.tag = &quot;sky&quot;, color = &quot;#ECECEC&quot;) hello_world_scene &lt;- a_scene(.template = &quot;empty&quot;, .children = list(box, sphere, cylinder, floor, backboard, sky), .js_sources = js_libs) ## Serve a scene hello_world_scene$serve() ## Stop serving a scene hello_world_scene$stop() This is the equivalent A-Frame scene: https://glitch.com/edit/#!/pricey-kitten Things to note: Components that were configured as HTML properties are now function arguments. r2vr has just one function for creating entities, a_entity(), that creates &lt;a-entity&gt; HTML. It can create the shorthand modes, eg &lt;a-box&gt;, using the .tag argument. The convention with argument names is anything that will appear in HTML literally is a plain argument, anything that is internal to r2vr has a . prefix. assets can be passed directly to entities, no need for the make the `#** id referencealthough assets still need ids. The Hadley spinnig uses the animation component. 9.4.2 R2VR tips r2vr is an A-Frame html code generator and server that users 3 main R6 classes created with a_entity(), a_asset() and a_scene(). You can view the HTML associated with these objects by calling their render() method. eg: 9.5 Spatial data in VR The type of work we will consider is making and plotting over 3D meshes. Recapping from the previous Act, the data types that are useful for this are: Rasters Digital Elevation Modes give us mesh heights, Images can give us textures Model output to shade meshes Simple features collections Giving us shapes for mesh boundaries The R packages we will use to get these data into VR are: raster sf tidyverse quashmesh r2vr.gis r2vr 9.5.1 DEM raster to VR For this example we will use a DEM dataset from Uluṟu-Kata Tut National Park. 9.5.1.1 Load Raster library(raster) library(quadmesh) uluru_raster &lt;- raster(&quot;./data/ELVIS_CLIP.tif&quot;) plot(uluru_raster) crs(uluru_raster) ## CRS arguments: ## +proj=lcc +lat_1=-30 +lat_2=-20 +lat_0=-25 +lon_0=135 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 Check: Does the raster have units=m? When we port the mesh to VR we want it to have units of meters since this is VCR’s native unit. Consider re-projecting if need be. 9.5.1.2 Crop Raster We’ll be doing a smaller section to make things faster library(r2vr.gis) library(sf) ### coords come from a google map: https://drive.google.com/open?id=1Ak26Hyx1R-f2QjPCTK0rQLye5xcHyE8n&amp;usp=sharing uluru_bbox &lt;- st_bbox(c(xmin = 131.02084, xmax = 131.0535, ymin = -25.35461, ymax = -25.33568), crs = st_crs(&quot;+proj=longlat +ellps=WGS84&quot;)) uluru_raster &lt;- raster_crop_bbox(uluru_raster, uluru_bbox) 9.5.1.3 Make Triangular Mesh We build a mesh using quadmesh and then cut each face in half so that it is a triangular mesh. A-Frame models can only have triangular faces. uluru_mesh &lt;- quadmesh(uluru_raster) rgl::shade3d(uluru_mesh) ## looks good? ## quadmesh::triangulate quads will make a mesh that VR thinks is inside out. ## This will be fixed in future. uluru_trimesh_indices &lt;- triangulate_quads(uluru_mesh$ib, clockwise = FALSE) We now have the pieces of a triangular mesh. uluru_mesh$vb - are the mesh vertices and are actual points in space. uluru_trimesh_indicies - are indices into the vertices that describe triangles. Because we had the quad mesh expressed in this primitive form the transformation could be made without creating any additional vertices. They are re-used for triangles. 9.5.1.4 Export to 3D model format The 3D model format we will use is a JSON format supported by three.js but not A-Frame natively. r2vr will take care of loading the 3rd party javascript necessary to use models of this type. When gltf support comes to R, that would be preferred, but until then this is what we have. Notice the use of t(). The VR tools expect columns of x, y, z coordinates while rgl and quadmesh work in rows. library(readr) uluru_json &lt;- trimesh_to_threejson(vertices = t(uluru_mesh$vb[1:3, ]), face_vertices = t(uluru_trimesh_indices)) write_file(uluru_json, &quot;uluru.json&quot;) 9.5.1.5 Render in A-Frame library(r2vr) uluru_asset &lt;- a_asset(id = &quot;uluru&quot;, src = &quot;uluru.json&quot;) uluru_model &lt;- a_json_model(src = uluru_asset) scene &lt;- a_scene(.template = &quot;basic_map&quot;, .children = list(uluru_model)) scene$serve() ## Fire started at 127.0.0.1:8080 a_kill_all_scenes() If you navigate to 127.0.0.1:8080 in your browser you should see the scene being served. You can try it on your phone as well but you need to use your computer’s public IP scene$serve: scene$serve(host = &quot;&lt;YOUR_IP&gt;&quot;) If things are working you should see a scene empty but for a grey grid. This grid is part of the &quot;basic_map&quot; template - it is added in automatically. It’s a visual reference as each square is 1x1 VR meters. It also let’s you know that things are ‘working’ - although Uluru is not visible at the moment. 9.5.1.6 Find Uluru We’ve imported a full scale model of Uluru in VR but we can’t see it just yet. Use the A-Frame inspector to find an appropriate, scale, position, rotation, and color, to view the model. Notice the position of the mesh, what does it say about how the coordinates have been transformed? 9.5.1.7 Setting the position and scale From the previous exercise we learned that the model was too large to practically view, was rotated with it’s z-axis pointing toward camera, and had been centred. To keep it spatially referenced, it’s a good idea to set up some constants relating to transforming the model to VR for example: scale_factor the scale of the model mesh_centre the centre of the original mesh so that we can use it to transform the coordinates of other things we would like to plot in spatial context over the mesh. height_correction the correction to apply to the height so that the ‘ground’ is at a VR height of 0. This means we need to decide on what the true ground height should be. Here we use a simple approach of taking the lowest point in the raster extent. We create this as an xyz vector so it can easily be added to positions. We define these and view the result: ## Model constants scale_factor &lt;- 0.01 mesh_centre &lt;- colMeans(t(uluru_mesh$vb[1:3,])) extent_coord_mat &lt;- matrix(extent(uluru_raster), nrow = 2, ncol = 2, byrow = FALSE) lowest_corner &lt;- min(raster::extract(uluru_raster, extent_coord_mat)) height_correction &lt;- c(0, mesh_centre[3] - lowest_corner, 0) ## Scene definition uluru_asset &lt;- a_asset(id = &quot;uluru&quot;, src = &quot;uluru.json&quot;, .parts = &quot;uluru.png&quot;) uluru_model &lt;- a_json_model(src = uluru_asset, scale = c(1, 1, 1) * scale_factor, position = (c(0, 0, -5) + height_correction) * scale_factor, material = list(color = &#39;#C88A77&#39;), rotation = c(-90, 0, 0)) scene &lt;- a_scene(.template = &quot;basic_map&quot;, .children = list(uluru_model)) scene$serve() a_kill_all_scenes() It’s taking shape! But we can do better than plain brown. 9.5.1.8 Texturing using satellite imagery We can use images to texture our mesh. This uses work we have done recently to make getting tiles from tile servers easier in R. The high level workflow is: Fetch and composite satellite tiles for the mesh bounding box. Rebuild mesh with quadmesh texture args, and export the model to supplying the texture coordinates with a reference to the texture image. 9.5.1.8.1 Fetch texture image ## Fetch textures from ESRI maps ## from slippymath README: library(purrr) library(curl) library(glue) library(slippymath) tile_grid &lt;- bb_to_tg(uluru_bbox, max_tiles = 15) esri_query_string &lt;- paste0(&quot;https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{zoom}/{y}/{x}&quot;) images &lt;- pmap(tile_grid$tiles, function(x, y, zoom){ outfile &lt;- glue(&quot;{x}_{y}.jpg&quot;) curl_download(url = glue(esri_query_string), destfile = outfile) outfile }, zoom = tile_grid$zoom) ## Create a new textured quamesh and export it to JSON raster_out &lt;- tg_composite(tile_grid, images) raster_to_png(raster_out, &quot;uluru.png&quot;) uluru_mesh_tex &lt;- quadmesh(uluru_raster, texture = raster_out, texture_filename = &quot;uluru.png&quot;) uluru_trimesh_indices &lt;- triangulate_quads(uluru_mesh_tex$ib, clockwise = FALSE) uluru_json &lt;- trimesh_to_threejson(vertices = t(uluru_mesh_tex$vb[1:3, ]), face_vertices = t(uluru_trimesh_indices), vertex_uvs = t(uluru_mesh_tex$texcoords), texture_file = &quot;uluru.png&quot;) write_file(uluru_json, &quot;uluru.json&quot;) ## Scene definition uluru_asset &lt;- a_asset(id = &quot;uluru&quot;, src = &quot;uluru.json&quot;, .parts = &quot;uluru.png&quot;) uluru_model &lt;- a_json_model(src = uluru_asset, scale = c(1, 1, 1) * scale_factor, position = (c(0, 0, -5) + height_correction) * scale_factor, rotation = c(-90, 0, 0), mesh_smooth = TRUE) scene &lt;- a_scene(.template = &quot;basic_map&quot;, .children = list(uluru_model)) scene$serve() a_kill_all_scenes() 9.5.1.9 Shading using data In this section we look at shading the mesh with an arbitrary raster. This could represent the output of a spatial model. We first generate a raster using a noise generator, we then use it to texture the mesh. library(ambient) library(scico) noise &lt;- setExtent(raster(noise_simplex(c(500, 600), fractal = &quot;none&quot;)), extent(uluru_raster)) colouring_raster_data &lt;- raster::extract(noise, t(uluru_mesh_tex$vb[1:2, ])) n_colours &lt;- 256 palette_function &lt;- purrr::partial(scico, palette = &quot;tokyo&quot;) vertex_colour_data &lt;- vec_pal_colours(colouring_raster_data, palette_function, n_colours, zero_index = TRUE) face_colours &lt;- vertex_to_face_colours(vertex_colour_data$indexes, t(uluru_trimesh_indices)) mesh_json &lt;- trimesh_to_threejson(vertices = t(uluru_mesh$vb[1:3,]), face_vertices = t(uluru_trimesh_indices), colours = vertex_colour_data$colours, face_vertex_colours = face_colours, transparency = 0.4) ## Scene definition uluru_asset_mod &lt;- a_in_mem_asset(id = &quot;uluru_mod&quot;, src = &quot;uluru_mod.json&quot;, .data = mesh_json) uluru_model2 &lt;- a_json_model(src = uluru_asset_mod, scale = c(1.001, 1.001, 1.001) * scale_factor, position = ((c(0, 0, -5) + height_correction) * scale_factor) + c(0, 0.01, 0), rotation = c(-90, 0, 0)) scene &lt;- a_scene(.template = &quot;basic_map&quot;, .children = list(uluru_model, uluru_model2)) scene$serve() a_kill_all_scenes() "]
]
